{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicing Policy Evaluation, Policy Improvement, Policy Iteration and Value Iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements.\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments. \n",
    "\n",
    "frozen_lake_env = gym.make('FrozenLake-v1')\n",
    "frozen_lake_env = frozen_lake_env.unwrapped.P\n",
    "\n",
    "slippery_bandit_walk = {\n",
    "    0: {\n",
    "        0: [(1.0,0,0.0,True)],\n",
    "        1: [(1.0,0,0.0,True)]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(0.8,0,0.0,True),(0.2,2,0.0,False)],\n",
    "        1: [(0.8,2,0.0,False),(0.2,0,0.0,True)]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(0.8,1,0.0,False),(0.2,3,0.0,False)],\n",
    "        1: [(0.8,3,0.0,False),(0.2,1,0.0,False)]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(0.8,2,0.0,False),(0.2,4,1.0,True)],\n",
    "        1: [(0.8,4,1.0,True),(0.2,2,0.0,False)]\n",
    "    },\n",
    "    4: {\n",
    "        0: [(1.0,4,0.0,True)],\n",
    "        1: [(1.0,4,0.0,True)]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation, an algorithm that allows us to compute the value function of a given policy. This allows us to compare policies. \n",
    "\n",
    "def policy_evaluation(pi, mdp, gamma=1, theta=1e-10):\n",
    "    ''' Iterates over the states of an MDP and computes the value according to the policy.\n",
    "        Returns the value function for the policy in the environment. '''\n",
    "\n",
    "    state_space = len(mdp)\n",
    "    state_values = np.zeros(state_space)\n",
    "\n",
    "    while True: # Until convergence.\n",
    "\n",
    "        current_values = np.zeros(state_space)\n",
    "\n",
    "        # Iterate over the states and compute the value of that state. \n",
    "        for state in range(state_space):\n",
    "\n",
    "            for transition_probability, state_prime, reward, done in mdp[state][pi[state]]: # Accessing the action that is taken in that state using policy pi. \n",
    "                current_values[state] += transition_probability * (reward + gamma * state_values[state_prime] * (not done))\n",
    "        \n",
    "        if np.max(np.absolute(state_values - current_values)) < theta:\n",
    "            break\n",
    "        \n",
    "        state_values = current_values.copy()\n",
    "\n",
    "    return current_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement, an algorithm that iterates over the actions in each state, computing the action-value. We then create our new policy by selecting greedily over the action-values in each state. \n",
    "\n",
    "def policy_constructor(Q_vals):\n",
    "    ''' Will construct the optimal policy given a matrix of Q values. '''\n",
    "\n",
    "    # enumerate returns an object of num: index values, we iterate over and construct the dict.\n",
    "    return {state:action for state, action in enumerate(np.argmax(Q_vals, axis=1))} \n",
    "\n",
    "def policy_improvement(value_function, mdp, gamma=1):\n",
    "    ''' Iterates over each state computing the action-value, returns an improved policy. '''\n",
    "\n",
    "    state_space = len(mdp)\n",
    "    action_space = len(mdp[0])\n",
    "    Q_vals = np.zeros((state_space, action_space), dtype=np.float64) # States = rows, actions = columns, ensuring its an integer. \n",
    "\n",
    "    for state in range(state_space):\n",
    "\n",
    "        for action in range(action_space):\n",
    "\n",
    "            for transition_probability, state_prime, reward, done in mdp[state][action]:\n",
    "                Q_vals[state][action] += transition_probability * (reward + gamma * value_function[state_prime] * (not done)) \n",
    "\n",
    "    improved_policy = policy_constructor(Q_vals)\n",
    "\n",
    "    return improved_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration, an algorithm that alternates between policy evaluation and policy improvement to render the best policy. \n",
    "\n",
    "def policy_iteration(mdp, gamma=1, theta=1e-10):\n",
    "    ''' Creates the optimal policy by iterating between PE and PI, \n",
    "        returns the best policy for an env. '''\n",
    "\n",
    "    state_space = len(mdp)\n",
    "    action_space = len(mdp[0])\n",
    "    value_function = None\n",
    "    \n",
    "    # Create an random policy.\n",
    "    current_policy = {state:action for state, action in enumerate(np.random.randint(0, action_space, size=state_space))}\n",
    "\n",
    "    while True:\n",
    "\n",
    "        policy_to_improve = current_policy\n",
    "\n",
    "        value_function = policy_evaluation(policy_to_improve, mdp, gamma, theta)\n",
    "\n",
    "        improved_policy = policy_improvement(value_function, mdp, gamma)\n",
    "\n",
    "        # Check if the new policy is equal to the old policy.\n",
    "        if policy_to_improve == {state:improved_policy[state] for state in range(state_space)}: # Because dictionaries are random, we cannot just use the == operator and compare.\n",
    "            break\n",
    "        \n",
    "        current_policy = improved_policy\n",
    "        \n",
    "    return improved_policy, value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration for Slippery Bandit Walk and Frozen Lake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBW Optimal Policy: {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}\n",
      "SBW Value Function: [0.         0.75294118 0.94117647 0.98823529 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slippery Bandit Walk\n",
    "sbw_optimal_policy, sbw_value_function = policy_iteration(slippery_bandit_walk)\n",
    "print(f\"SBW Optimal Policy: {sbw_optimal_policy}\")\n",
    "print(f\"SBW Value Function: {sbw_value_function}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL Optimal Policy: {0: 0, 1: 3, 2: 3, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 3, 9: 1, 10: 0, 11: 0, 12: 0, 13: 2, 14: 1, 15: 0}\n",
      "FL Value Function: [0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frozen Lake\n",
    "fl_optimal_policy, fl_value_function = policy_iteration(frozen_lake_env)\n",
    "print(f\"FL Optimal Policy: {fl_optimal_policy}\")\n",
    "print(f\"FL Value Function: {fl_value_function}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_constructor(Q_vals):\n",
    "    ''' Will construct the optimal policy given a matrix of Q values. '''\n",
    "\n",
    "    # Enumerate returns an object of num: index values, we iterate over and construct the dict.\n",
    "    return {state:action for state, action in enumerate(np.argmax(Q_vals, axis=1))} \n",
    "\n",
    "\n",
    "def value_iteration(mdp, gamma=1, theta=1e-10):\n",
    "    ''' Performs value iteration, this does not use policy evaluation or iteration. '''\n",
    "\n",
    "    state_space = len(mdp)\n",
    "    action_space = len(mdp[0])\n",
    "    value_function = np.zeros(state_space)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        Q_vals = np.zeros((state_space, action_space), dtype=np.float64)\n",
    "\n",
    "        for state in range(state_space):\n",
    "\n",
    "            for action in range(action_space):\n",
    "\n",
    "                for transition_probability, state_prime, reward, done in mdp[state][action]:\n",
    "                    Q_vals[state][action] += transition_probability * (reward + gamma * value_function[state_prime] * (not done))\n",
    "\n",
    "        if np.max(np.absolute(value_function - np.max(Q_vals, axis=1))) < theta:\n",
    "            break\n",
    "    \n",
    "        value_function = np.max(Q_vals, axis=1)\n",
    "\n",
    "    policy = policy_constructor(Q_vals)\n",
    "\n",
    "    return policy, value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration on Slippery Bandit Walk and Frozen Lake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBW Optimal Policy: {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}\n",
      "SBW Value Function: [0.         0.75294118 0.94117647 0.98823529 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sbw_optimal_policy, sbw_value_function = value_iteration(slippery_bandit_walk)\n",
    "print(f\"SBW Optimal Policy: {sbw_optimal_policy}\")\n",
    "print(f\"SBW Value Function: {sbw_value_function}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL Optimal Policy: {0: 0, 1: 3, 2: 3, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 3, 9: 1, 10: 0, 11: 0, 12: 0, 13: 2, 14: 1, 15: 0}\n",
      "FL Value Function: [0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frozen Lake\n",
    "fl_optimal_policy, fl_value_function = policy_iteration(frozen_lake_env)\n",
    "print(f\"FL Optimal Policy: {fl_optimal_policy}\")\n",
    "print(f\"FL Value Function: {fl_value_function}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Comparison Between Policy Iteration and Value Iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.3 ms ± 3.15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "51.2 ms ± 553 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit policy_iteration(frozen_lake_env)\n",
    "%timeit value_iteration(frozen_lake_env)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4a9bf1d79eb24c427ec85c37d8d67eb010d74f3ee5dc8712a538391781d73a7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
